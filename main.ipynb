{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If working in colab\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU transformers accelerate bitsandbytes rich sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/winstor/clones/chatbot-nlp/main.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/winstor/clones/chatbot-nlp/main.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/winstor/clones/chatbot-nlp/main.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPY007/TinyLlama-1.1B-Chat-v0.3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/winstor/clones/chatbot-nlp/main.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model)\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/winstor/clones/chatbot-nlp/main.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pipeline \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mpipeline(\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/winstor/clones/chatbot-nlp/main.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/winstor/clones/chatbot-nlp/main.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/winstor/clones/chatbot-nlp/main.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/winstor/clones/chatbot-nlp/main.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m/mnt/stor/miniconda/envs/hf/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:751\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    749\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_candidate\u001b[39m}\u001b[39;00m\u001b[39m does not exist or is not currently imported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    750\u001b[0m         )\n\u001b[0;32m--> 751\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    753\u001b[0m \u001b[39m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[39m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m/mnt/stor/miniconda/envs/hf/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2017\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2015\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2017\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   2018\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   2019\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   2020\u001b[0m     init_configuration,\n\u001b[1;32m   2021\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   2022\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2023\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2024\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2025\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   2026\u001b[0m     _is_local\u001b[39m=\u001b[39;49mis_local,\n\u001b[1;32m   2027\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2028\u001b[0m )\n",
      "File \u001b[0;32m/mnt/stor/miniconda/envs/hf/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2249\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m \u001b[39m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2248\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2249\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   2250\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   2251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   2252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2253\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2254\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/stor/miniconda/envs/hf/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama_fast.py:122\u001b[0m, in \u001b[0;36mLlamaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    111\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    121\u001b[0m ):\n\u001b[0;32m--> 122\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    123\u001b[0m         vocab_file\u001b[39m=\u001b[39;49mvocab_file,\n\u001b[1;32m    124\u001b[0m         tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    125\u001b[0m         clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m    126\u001b[0m         unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    127\u001b[0m         bos_token\u001b[39m=\u001b[39;49mbos_token,\n\u001b[1;32m    128\u001b[0m         eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    129\u001b[0m         use_default_system_prompt\u001b[39m=\u001b[39;49muse_default_system_prompt,\n\u001b[1;32m    130\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    131\u001b[0m     )\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_bos_token \u001b[39m=\u001b[39m add_bos_token\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_eos_token \u001b[39m=\u001b[39m add_eos_token\n",
      "File \u001b[0;32m/mnt/stor/miniconda/envs/hf/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:111\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[1;32m    109\u001b[0m \u001b[39melif\u001b[39;00m fast_tokenizer_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m from_slow:\n\u001b[1;32m    110\u001b[0m     \u001b[39m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m TokenizerFast\u001b[39m.\u001b[39;49mfrom_file(fast_tokenizer_file)\n\u001b[1;32m    112\u001b[0m \u001b[39melif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[39m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers \n",
    "import torch\n",
    "model = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse(prompt,pipeline):\n",
    "    \n",
    "    formatted_prompt = (\n",
    "        f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "    CHAT_EOS_TOKEN_ID = 32002\n",
    "\n",
    "    sequences = pipeline(\n",
    "        formatted_prompt,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p = 0.9,\n",
    "        num_return_sequences=1,\n",
    "        repetition_penalty=1.1,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=CHAT_EOS_TOKEN_ID,\n",
    "    )\n",
    "    \n",
    "    out=[]\n",
    "    for seq in sequences:\n",
    "        rprint(f\"Result: {seq['generated_text']}\")\n",
    "        out.append(f\"Result: {seq['generated_text']}\")\n",
    "        \n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32002 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Result: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">|im_start|</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;user</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">How do i write a program to print hello world in rust&lt;|im_end|&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;|im_start|&gt;assistant</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To write a program in Rust that will print </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Hello World\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> to the terminal, you can use the following steps:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Create a new Rust module called `main` with the following code:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```rust</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">fn </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">main</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    println!</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Hello World!\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Create an executable file named `hello_world.exe` using the `rustc` command line tool. You can do this by running </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the following command in your terminal:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```bash</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">rustc -vF hello_world.rs </span><span style=\"font-weight: bold\">&gt;</span> hello_world.txt\n",
       "```\n",
       "Copy the contents of the `hello_world.txt` file into your desired location for execution. For example, if you want \n",
       "to execute the program on a system that has `hello_world.exe` installed, you would run the following command:\n",
       "```bash\n",
       ".<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">hello_world.exe</span>\n",
       "```\n",
       "You should see the <span style=\"color: #008000; text-decoration-color: #008000\">\"Hello World!\"</span> message displayed on the terminal.\n",
       "This is just one way to create and execute a Rust program, but there are many other ways as well. The key point to \n",
       "keep in mind is that Rust is a statically-typed language, so any changes to the code would require recompiling and \n",
       "loading the changes into memory.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Result: \u001b[1m<\u001b[0m\u001b[1;95m|im_start|\u001b[0m\u001b[39m>user\u001b[0m\n",
       "\u001b[39mHow do i write a program to print hello world in rust<|im_end|>\u001b[0m\n",
       "\u001b[39m<|im_start|>assistant\u001b[0m\n",
       "\u001b[39mTo write a program in Rust that will print \u001b[0m\u001b[32m\"Hello World\"\u001b[0m\u001b[39m to the terminal, you can use the following steps:\u001b[0m\n",
       "\n",
       "\u001b[39mCreate a new Rust module called `main` with the following code:\u001b[0m\n",
       "\u001b[39m```rust\u001b[0m\n",
       "\u001b[39mfn \u001b[0m\u001b[1;35mmain\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m    println!\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"Hello World!\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m;\u001b[0m\n",
       "\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39mCreate an executable file named `hello_world.exe` using the `rustc` command line tool. You can do this by running \u001b[0m\n",
       "\u001b[39mthe following command in your terminal:\u001b[0m\n",
       "\u001b[39m```bash\u001b[0m\n",
       "\u001b[39mrustc -vF hello_world.rs \u001b[0m\u001b[1m>\u001b[0m hello_world.txt\n",
       "```\n",
       "Copy the contents of the `hello_world.txt` file into your desired location for execution. For example, if you want \n",
       "to execute the program on a system that has `hello_world.exe` installed, you would run the following command:\n",
       "```bash\n",
       ".\u001b[35m/\u001b[0m\u001b[95mhello_world.exe\u001b[0m\n",
       "```\n",
       "You should see the \u001b[32m\"Hello World!\"\u001b[0m message displayed on the terminal.\n",
       "This is just one way to create and execute a Rust program, but there are many other ways as well. The key point to \n",
       "keep in mind is that Rust is a statically-typed language, so any changes to the code would require recompiling and \n",
       "loading the changes into memory.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out=converse(\"How do i write a program to print hello world in rust\",pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
